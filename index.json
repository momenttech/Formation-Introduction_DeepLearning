{
	"pages": [
		{"title": "Formation au Deep-Learning", "text": "Formation au Deep-Learning\nSommaire\n\nRéseau de neurones\nDroites de régression\nListes de matrices\nCalcul d’erreur\n\nLes matrices : quelques rappels !\nL&#39;addition de matrice\nGénéralités :\n\nL&#39;addition des matrices est définie pour deux matrices de même type.\nLa somme de deux matrices de type (m, n), est obtenue en additionnant les éléments correspondants.\n\nExemple avec :\n[\n\\begin{Bmatrix}\n   A_{1}  \\\n   A_{2}\n\\end{Bmatrix}\n] + [\n\\begin{Bmatrix}\n   B_{1}  \\\n   B_{2}\n\\end{Bmatrix}\n]\nLe produit matriciel\nGénéralités :\n\nLa multiplication des matrices n&#39;est pas commutative, c&#39;est-à-dire que AB n&#39;est pas égal à BA.\n\nÀ l’avenir, nous aurons seulement besoin de connaitre le produit d’une matrice de type (1,2) et d’une matrice de type (2,2).\nExemple avec :\n[\n\\begin{Bmatrix}\n   A_{1}  \\\n   A_{2}\n\\end{Bmatrix}\n] + [\n\\begin{Bmatrix}\n   B_{1} &amp;&amp; B_{1,2}  \\\n   B_{2,1} &amp;&amp; B_{2,2}\n\\end{Bmatrix}\n]\nRéseau de neurones\nLe réseau fully connected\nDéfinition :\n\nLe neurone formel est conçu comme un automate doté d&#39;une fonction de transfert qui transforme ses entrées en sortie selon des règles précises. Ces neurones sont par ailleurs associés en réseaux dont la topologie des connexions est variable : réseaux proactifs, récurrents, etc..\n\n\n\nUn réseau de neurones est considéré « fully connected » lorsque toute entrée est relié par une arête appelé « poids » et représenté par «  » à l’intégralité des neurones présents dans les couches cachées.\n\n\nReprésentation graphique :\n\n[ ! ] Les couches cachées sont appelé zone de « pré-activation » et l’ensemble des output zone d’ « activation ».\nDéfinition d&#39;un biais :\n\nLe biais est l&#39;erreur provenant d’hypothèses erronées dans l&#39;algorithme d&#39;apprentissage. Un biais élevé peut être lié à un algorithme qui manque de relations pertinentes entre les données en entrée et les sorties prévues (sous-apprentissage).\n\nDéfinition d’une fonction d’activation :\n\nLa fonction d’activation (ou fonction de seuillage, ou encore fonction de transfert) sert à introduire une non-linéarité dans le fonctionnement du neurone.\nLes fonctions de seuillage présentent généralement trois intervalles :\nen dessous du seuil, le neurone est non-actif\naux alentours du seuil, une phase de transition\nau-dessus du seuil, le neurone est actif\n\n\n\nCalcul de la valeur d&#39;un neurone :\nDroites de régression\nRégression linéaire\nDéfinition :\n\nDésigne un modèle dans lesquels est la médiane conditionnelle de « y » sachant « x ».\nLe modèle de régression linéaire est souvent estimé par la méthode des moindres carrés mais il existe aussi de nombreuses autres méthodes pour estimer ce modèle.\n\nReprésentation graphique du réseau précèdent :\n\nCe schéma représente ainsi la fonction d’activation\nLa fonction d&#39;activation\nDéfinition :\n\nLa fonction d’activation est une fonction mathématique appliquée à un signal en sortie d&#39;un neurone artificiel. Soit dans notre cas à la droite de régression linéaire.\n\nGraphiquement :\nCas pratique :\n\nplayground.tensorflow.org\n\n\nListes de matrices\nMatrice : définition\nDéfinition :\n\nLes matrices sont des tableaux de nombres qui servent à interpréter en termes calculatoires et donc opérationnels les résultats théoriques de l&#39;algèbre\n\n\nPrenons pour exemple, cette image :\n\nCalculs par CPU / GPU / TPU\nCPU versus GPU\nLe facteur nombre de cœurs :\n\n\nAvantages :\nAccélération via GPU des applications\n\n\n\n\nDémonstration :\n\n\nhttps://www.youtube.com/watch?v=-P28LKWTzrI\nTPU ? Késako ?\nDéfinition :\n\nLe TPU (Tensor Processor Unit) est un module hardware dédié spécifiquement aux applications de Machine Learning\n\n\nCalcul d&#39;erreur\nNotion d&#39;erreur\nDéfinition :\n\nA chaque itération, l&#39;algorithme va calculer un indicateur de performance globale (l&#39;erreur qu&#39;il commet) en comparant la sortie attendue et la sortie prédite.\n\n\nLe batch\nDéfinition :\nLe minimum local\nDéfinition :\n\nLe minimum local est point dans une zone où le système établit qu’il ne peut semble pense avoir obtenu la meilleure précision mais ne l’est effectivement pas sur la courbe de précision de classification.\n\n\nLe learning rate\nDéfinition :\n\nReprésente la taille du « pas » en avant effectuer par le système pour atteindre le point d’apprentissage le plus efficient\n\n\n\nLe vanishing gradients\n\nDéfinition :\n\nLe vanishing gradients est une perte (ou fuite) de gradient, affectant les neurones plus profond et unités de saturations dans un réseau profond.\n\n\nL&#39;overfitting\nDéfinition :\n\nL’overfitting (ou surapprentissage) est une étape où le système est arrivé à reconnaitre quasi-seulement les images sur lesquelles il a été entrainé et une variation de lumière ou de milieu peut l’induire à ne pas reconnaitre l’objet.\n\n\n", "tags": "", "url": "Introduction_au_Deep_Learning.html"},
		{"title": "Formation au Deep-Learning", "text": "Formation au Deep-Learning\n\nSommaire\n\nLes réseaux de neurones\nLes matrices : quelques rappels\nDroites de régression\nListes de matrices\nCalcul par CPU / GPU / TPU\nCalcul d’erreur\n\n\nLes Réseau de Neurones\n\nLe réseau fully connected\n\nLe réseau fully connected\n\nUn réseau de neurones est considéré « fully connected » lorsque toute entrée est relié par une arête appelé « poids » et représenté par « wi,j » à l’intégralité des neurones présents dans les couches cachées.\n\n\nLe réseau fully connected\nL&#39;équation du neurone :\nSortie du Neurone = X1W1,1 + X2W2,1 + B1 = Régression lineaire\n \n\nLe réseau fully connected\nAvec plusieurs neurones :\n\nPour calculer la valeur d’un neurone, il faut effectuer la somme des connexions entrantes :\nNeurone 1 = X1W1,1 + X2W2,1 + B1Neurone 2 = X1W1,2 + X2W2,2 + B2\nLe réseau fully connected\nVue matricielle :\n\n\nLe réseau fully connected\nLa Fonction d’Activation :\n(ou fonction de seuillage, ou encore fonction de transfert)\n\nPrésente à la sortie du neurone.\nElle répond à trois exigences:\nNon linéaire   -&gt; Pour modéliser des fonctions complexes (non linéaires)\nDifférentielle -&gt; Pour permetre la retro-propagation de l&#39;erreur\nMonotonique    -&gt; Pour éviter de rajouter des minimums locaux\n\n\n\n\n\nLes matrices : quelques rappels !\n\nL&#39;addition de matrice\n\nL&#39;addition de matrice\n&nbsp;\n\n\nGénéralités :\n\nL&#39;addition des matrices est définie pour deux matrices de même type.  \n\n\nLa somme de deux matrices de type (m, n), est obtenue en additionnant les éléments correspondants.\n\n\nL&#39;addition de matrice\n&nbsp;\n\nExemple avec :\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 1 :\n&nbsp;\n\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nLe produit matriciel\n\nLe produit matriciel\nGénéralités :\n\nLa multiplication des matrices n&#39;est pas commutative, c&#39;est-à-dire que A&bull;B n&#39;est pas égal à B&bull;A.\n\nÀ l’avenir, nous aurons seulement besoin de connaître le produit d’une matrice de type (1,2) et d’une matrice de type (2,2).\n&nbsp;\n\n[!] Produit matriciel &ne; multiplication de matrice\nLe produit matriciel\nExemple avec :\n\nLe produit matriciel\n&nbsp;\n\nÉtape 1 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 3 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 4 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 5 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 6 :\n&nbsp;\n\n\n\nDroites de régression\n\nRégression linéaire\n\nRégression linéaire\nDéfinition :\n\nDésigne un modèle dans lesquels est la médiane conditionnelle de « y » sachant « x ».\n\nLe modèle de régression linéaire est souvent estimé par la méthode des moindres carrés mais il existe aussi de nombreuses autres méthodes pour estimer ce modèle.\n\n\nRégression linéaire\nReprésentation graphique du réseau précèdent :\nVoici une droite de régression linéaire. Par définition elle suit l&#39;équation suivante :ax + bAfin de mieux comprendre et de l&#39;appliquer à notre réseau de neurone, nous pouvons l&#39;écrire de la façon suivante :xw + B  \nx : la valeur d&#39;entréew : le poidsB : le biais\n\nRégression linéaire\nReprésentation graphique du réseau précèdent :\n\nCe schéma représente ainsi la fonction d’activation\nLa fonction d&#39;activation\n\nLa fonction d&#39;activation\nDéfinition :\n\nLa fonction d’activation est une fonction mathématique appliquée à un signal en sortie d&#39;un neurone artificiel. Soit dans notre cas à la droite de régression linéaire.\n\nLa fonction d&#39;activation\nGraphiquement :\n\nLa fonction d&#39;activation\nCas pratique :\n\nhttp://playground.tensorflow.org\n\n\n\nListes de matrices\n\nUne matrice\n\nUne matrice\nDéfinition :\n\nUne matrice est une liste de listes, une liste est une liste de vecteurs, un vecteur est une liste de chiffres.\n\n\nUne matrice\nPrenons pour exemple, cette image :\n\nUne matrice\nPrenons pour exemple, cette image :\n\n  \n    \n    Détails image :     - Dimensions :       1280 x 768     - Caractéristiques :       En couleurs (3 dimensions)\n    \n  \n\n\n\nCalculs par CPU / GPU / TPU\n\nCPU versus GPU\n\nCPU versus GPU\nLe facteur nombre de cœurs :\n\nCPU versus GPU\n\nAvantages :\nAccélération via GPU des applications\n\n\n\n\nCPU versus GPU\nDémonstration :\n\n\nhttps://www.youtube.com/watch?v=-P28LKWTzrI\nTPU ? Késako ?\n\nTPU ? Késako ?\nDéfinition :\n\nLe TPU (Tensor Processor Unit) est un module hardware dédié spécifiquement aux applications de Machine Learning\n\n\n\nCalcul d&#39;erreur\n\nNotion d&#39;erreur\n\nNotion d&#39;erreur\nDéfinition :\n\nA chaque itération, l&#39;algorithme va calculer un indicateur de performance globale (l&#39;erreur qu&#39;il commet) en comparant la sortie attendue et la sortie prédite.\n\n\nLe batch\n\nLe batch\nDéfinition :\nLe gradient descent\n\nLe gradient descent\nDéfinition :\nLe learning rate\n\nLe learning rate\nDéfinition :\n\nReprésente la taille du « pas » en avant effectuer par le système pour atteindre le point d’apprentissage le plus efficient\n\n\n\nLe minimum local\n\nLe minimum local\nDéfinition :\n\nLe minimum local est point dans une zone où le système établit qu’il ne peut semble pense avoir obtenu la meilleure précision mais ne l’est effectivement pas sur la courbe de précision de classification.\n\n\nNotion de dérivée de sigmoïde\n\nNotion de dérivée de sigmoïde\nDéfinition :\nNotion de dérivée de sigmoïde\nDéfinition (suite) :\nLe vanishing gradients\n\n\nLe vanishing gradients\nDéfinition :\n\nLe vanishing gradients est une perte (ou fuite) de gradient, affectant les neurones plus profond et unités de saturations dans un réseau profond.\n\n\nL&#39;overfitting\n\nL&#39;overfitting\nDéfinition :\n\nL’overfitting (ou surapprentissage) est une étape où le système est arrivé à reconnaitre quasi-seulement les images sur lesquelles il a été entrainé et une variation de lumière ou de milieu peut l’induire à ne pas reconnaitre l’objet.\n\n\nLe cross-validation\nDéfinition :\n\nLa validation croisée (cross-validation) est une méthode d’estimation de fiabilité d&#39;un modèle fondé sur une technique d&#39;échantillonnage. Cela sert à comparer la pertinence d&#39;un modèle par rapport à un autre.\n\n\nRéseau neuronal à convolution\n\nLes convolutions\n\nLes convolutions\nDéfinition :\n\nLes convolutions consistent en un empilage multicouche d&#39;algorithme, dont le but est de pré-traiter de petites quantités d&#39;informations.\n\nLes convolutions\nCas concret :\n\nLes blocs de construction\n\nLes blocs de construction\nDéfinition :\nUne architecture de réseau de neurones convolutifs est formée par un empilement de couches de traitement (blocs de construction), il en existe 5 :\n\nla couche de convolution (CONV)\nla couche de pooling (POOL)\nla couche de correction (ReLU)\nla couche « entièrement connectée » (FC)\nla couche de perte (LOSS)\n\nLa couche de pooling\n\nLa couche de pooling\nDéfinition :\n\nLe pooling (« mise en commun »), est une forme de sous-échantillonnage de l&#39;image.\nLe pooling réduit la taille spatiale d&#39;une image, réduisant ainsi la quantité de paramètres et de calcul dans le réseau. Il est donc fréquent d&#39;insérer périodiquement une couche de pooling entre deux couches convolutives successives d&#39;une architecture de réseau de neurones convolutifs pour réduire le sur-apprentissage.\n\nLa couche de pooling\n[ ! ] Il existe plusieurs méthodes afin de réduire la taille spatiale d&#39;une image concernant le pooling :\n\nAverage pooling\nMax-pooling\nL2-norm pooling\nStocastic pooling\n\nLe max-pooling\n\nLe max-pooling\nDéfinition :\n\nLe max-pooling permet une réduction de la taille de la représentation en gardant seulement la plus grande valeur des tuiles dans le filtre.\n\n\nLe max-pooling\nDétaillons :\n\n\nIci, nous avons un filtre de 2 x 2, avec un pas de 2  \n\n(il est possible d&#39;avoir un filtre plus important, ou encore, de ne pas avoir de couche de pooling)\n\nContact\n&nbsp;\n&nbsp;\n\n\nTom DESHAIRES - MomentTech SAS\n@: \ntom.deshaires@mmtt.fr\n\n&nbsp;\n&nbsp;\n&nbsp;\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License\n\nRessources\n\nWikipédia\n TensorFlow Playground \n EPITA \n Microsoft \n Le deep-learning pas à pas \n Convolutional Neural Networks for Visual Recognition \n Wingshore \n Quora \n\n", "tags": "", "url": "index.html"},
		{"title": "Formation au Deep-Learning", "text": "Formation au Deep-Learning\n\nSommaire\n\nRéseau de neurones\nDroites de régression\nListes de matrices\nCalcul d’erreur\n\nLes matrices : quelques rappels !\nL&#39;addition de matrice\n\nGénéralités :\n\nL&#39;addition des matrices est définie pour deux matrices de même type.\nLa somme de deux matrices de type (m, n), est obtenue en additionnant les éléments correspondants.\n\nL&#39;addition de matrice\n&nbsp;\n\nExemple avec :\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 1 :\n&nbsp;\n\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nLe produit matriciel\n\nLe produit matriciel\nGénéralités :\n\nLa multiplication des matrices n&#39;est pas commutative, c&#39;est-à-dire que AB n&#39;est pas égal à BA.\n\nÀ l’avenir, nous aurons seulement besoin de connaitre le produit d’une matrice de type (1,2) et d’une matrice de type (2,2).\nLe produit matriciel\nExemple avec :\n\nLe produit matriciel\n&nbsp;\n\nÉtape 1 :\n&nbsp;\n\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nL&#39;addition de matrice\n&nbsp;\n\nÉtape 2 :\n&nbsp;\n\n\nLe produit matriciel\n&nbsp;\n\nÉtape 3 :\n&nbsp;\n\n\nLe produit de matrice\n&nbsp;\n\nÉtape 4 :\n&nbsp;\n\n\nLe produit de matrice\n&nbsp;\n\nÉtape 5 :\n&nbsp;\n\n\nLe produit de matrice\n&nbsp;\n\nÉtape 6 :\n&nbsp;\n\n\n", "tags": "", "url": "Trash/AdditionEtProduitDeMatrice.html"},
		{"title": "Calculs par CPU / GPU / TPU", "text": "Calculs par CPU / GPU / TPU\nCPU versus GPU\n\nCPU versus GPU\nLe facteur nombre de cœurs :\n\nCPU versus GPU\n\nAvantages :\nAccélération via GPU des applications\n\n\n\n\nCPU versus GPU\nDémonstration :\n\n\nhttps://www.youtube.com/watch?v=-P28LKWTzrI\nTPU ? Késako ?\n\nTPU ? Késako ?\nDéfinition :\n\nLe TPU (Tensor Processor Unit) est un module hardware dédié spécifiquement aux applications de Machine Learning\n\n\n", "tags": "", "url": "Trash/CalculCpuGpuTpu.html"},
		{"title": "Calcul d&#x27;erreur", "text": "Calcul d&#39;erreur\nNotion d&#39;erreur\n\nNotion d&#39;erreur\nDéfinition :\n\nA chaque itération, l&#39;algorithme va calculer un indicateur de performance globale (l&#39;erreur qu&#39;il commet) en comparant la sortie attendue et la sortie prédite.\n\n\nLe batch\n\nLe batch\nDéfinition :\nLe minimum local\n\nLe minimum local\nDéfinition :\n\nLe minimum local est point dans une zone où le système établit qu’il ne peut semble pense avoir obtenu la meilleure précision mais ne l’est effectivement pas sur la courbe de précision de classification.\n\n\nLe learning rate\n\nLe learning rate\nDéfinition :\n\nReprésente la taille du « pas » en avant effectuer par le système pour atteindre le point d’apprentissage le plus efficient\n\n\n\nLe vanishing gradients\n\n\nLe vanishing gradients\nDéfinition :\n\nLe vanishing gradients est une perte (ou fuite) de gradient, affectant les neurones plus profond et unités de saturations dans un réseau profond.\n\n\nL&#39;overfitting\n\nL&#39;overfitting\nDéfinition :\n\nL’overfitting (ou surapprentissage) est une étape où le système est arrivé à reconnaitre quasi-seulement les images sur lesquelles il a été entrainé et une variation de lumière ou de milieu peut l’induire à ne pas reconnaitre l’objet.\n\n\n", "tags": "", "url": "Trash/CalculErreur.html"},
		{"title": "Contact", "text": "Contact\n&nbsp;\n&nbsp;\n\nTom DESHAIRES - MomentTech SAS\n@: \ntom.deshaires@mmtt.fr\n\n&nbsp;\n&nbsp;\n&nbsp;\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License\n", "tags": "", "url": "Trash/Contact.html"},
		{"title": "Droites de régression", "text": "Droites de régression\nRégression linéaire\n\nRégression linéaire\nDéfinition :\n\nDésigne un modèle dans lesquels est la médiane conditionnelle de « y » sachant « x ».\n\n&nbsp;\n\n\nLe modèle de régression linéaire est souvent estimé par la méthode des moindres carrés mais il existe aussi de nombreuses autres méthodes pour estimer ce modèle.\n\nRégression linéaire\nReprésentation graphique du réseau précèdent :\n\nCe schéma représente ainsi la fonction d’activation\nLa fonction d&#39;activation\n\nLa fonction d&#39;activation\nDéfinition :\n\nLa fonction d’activation est une fonction mathématique appliquée à un signal en sortie d&#39;un neurone artificiel. Soit dans notre cas à la droite de régression linéaire.\n\nLa fonction d&#39;activation\nGraphiquement :\nLa fonction d&#39;activation\nCas pratique :\n\nhttp://playground.tensorflow.org\n\n\n", "tags": "", "url": "Trash/DroitesRegre.html"},
		{"title": "Listes de matrices", "text": "Listes de matrices\nUne matrice\n\nUne matrice\nDéfinition :\n\nUne matrice est un tableau de nombres qui sert à interpréter en termes calculatoires et donc opérationnels les résultats théoriques de l&#39;algèbre\n\n\nUne matrice\nPrenons pour exemple, cette image :\n\n", "tags": "", "url": "Trash/ListeDeMatrice.html"},
		{"title": "Réseau de neurones", "text": "Réseau de neurones\nLe réseau fully connected\n\nLe réseau fully connected\nDéfinition :\n\nLe neurone formel est conçu comme un automate doté d&#39;une fonction de transfert qui transforme ses entrées en sortie selon des règles précises. Ces neurones sont par ailleurs associés en réseaux dont la topologie des connexions est variable : réseaux proactifs, récurrents, etc..\n\n\nLe réseau fully connected\n\nUn réseau de neurones est considéré « fully connected » lorsque toute entrée est relié par une arête appelé « poids » et représenté par « w » à l’intégralité des neurones présents dans les couches cachées.\n\n\nLe réseau fully connected\nReprésentation graphique :\n\n[ ! ] Les couches cachées sont appelé zone de « pré-activation » et l’ensemble des output zone d’ « activation ».\nLe réseau fully connected\nDéfinition d&#39;un biais :\n\nLe biais est l&#39;erreur provenant d’hypothèses erronées dans l&#39;algorithme d&#39;apprentissage. Un biais élevé peut être lié à un algorithme qui manque de relations pertinentes entre les données en entrée et les sorties prévues (sous-apprentissage).\n\nLe réseau fully connected\nDéfinition d’une fonction d’activation :\n\nLa fonction d’activation (ou fonction de seuillage, ou encore fonction de transfert) sert à introduire une non-linéarité dans le fonctionnement du neurone.\n\n&nbsp;\n\n\nLes fonctions de seuillage présentent généralement trois intervalles :\nen dessous du seuil, le neurone est non-actif\naux alentours du seuil, une phase de transition\nau-dessus du seuil, le neurone est actif\n\n\n\nLe réseau fully connected\nCalcul de la valeur d&#39;un neurone :\n", "tags": "", "url": "Trash/ReseauDeNeuronnes.html"},
		{"title": "image", "text": "\n\n\n\n\n\n\n\n\n\n\n\n", "tags": "", "url": "Trash/image.html"},
		{"title": "Markdown Demo", "text": "Markdown Demo\nExternal 1.1\nContent 1.1\nNote: This will only appear in the speaker notes window.\nExternal 1.2\nContent 1.2\nExternal 2\nContent 2.1\nExternal 3.1\nContent 3.1\nExternal 3.2\nContent 3.2\n", "tags": "", "url": "website/assets/js/reveal/plugin/markdown/example.html"}
	]
}
